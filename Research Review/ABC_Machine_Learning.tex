\documentclass[12pt,a4paper]{article}
\author{Xinyu Zhong\\Wolfson College}
\usepackage{physics, amsmath}
\usepackage{xcolor}
\usepackage[margin=0.5in]{geometry}

\title{Notes}
%\date{2nd Nov 2021}

\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhf{}
\lhead{\fancyplain{}{Xinyu Zhong, xz447@cam.ac.uk}}
\rhead{\fancyplain{}{ABC to Machine Learning}}
\cfoot{\fancyplain{}{\thepage{}}}
\setlength {\headheight}{15pt}

\newcommand{\definition}[3]
    {
    \textit{Definition #1: }
    \begin{center}
        {#2}
    \end{center}
    {#3}\\
    }
\newcommand{\theorem}[2]{\textbf{\textcolor{red}{#1: }}\textcolor{red}{#2}}
\newcommand{\example}[1]{\par\textbf{Example: }\textcolor{blue}{#1}}

\begin{document}

\begin{titlepage}
    \maketitle
\end{titlepage}

\tableofcontents

\newpage

\begin{abstract}
\noindent
Abstract of this course
\end{abstract}
\section{Introduction}
mention supervised vs. unsupervised, supervised is more relavent for physics.
\section{Overview of fitting}
\subsection{Fitting techniques}
\paragraph*{}
Although the idea of machine learning is first introduced in 1959,
% https://citeseerx.ist.psu.edu/doc/10.1.1.368.2254
the most fundamental technique involved in machine learning dates way back in history when man explore ways to find the best method of curve fitting.
Curve fitting is a process of construction a curve, or mathematics function, that has the best fit to a series of data points. It still remains as one of the most theoretically challenging part of machine learning.
% https://towardsdatascience.com/a-deep-dive-into-curve-fitting-for-ml-7aeef64755d2
\paragraph{Linear regression}
The most basic, and commonly seen fitting technique is a first order polynomial equation:
\begin{equation}
    y=ax+b
\end{equation}
which is a straight line that connects two points with distinct x coordinates. This is also known as linear regression.
\paragraph{Taylor Theorem}
With 3 data to fit,  we could always add a term of higher power of $x$, to make it a quadratic equation
\begin{equation}
    y= ax^2 +bx+ c
\end{equation}
or another term too construct a cubic regression:
\begin{equation}
    y= ax^3 +bx^2 + cx+d
\end{equation}
The objective is to minimise the ordinary least squares:
\begin{equation}
    sum{y_i-(kx_i+c)}^2
\end{equation}
%Ideals:
%Zachary Taylor's answer about Taylor's theorm and tis limitations
This reminds us a 
Taylor expansion only works for small x, disaster at large x.
The limitation of this Taylor expansion comes when the x becomes an infinitely large value, which will cause the magnitude of $y$ to become infinitely large, which many not reflects the datasets properly. 
Another limitation comes in when the number of independent variables becomes more than 1. For example, $y$ is now a function of $x_1$ and $x_2$. i.e. $y(x_1,x_2)$.
Taylor series cannot extrapolate the function for then the independet variable x becomes large
In this case, we would have to include a term such as $x_1x_2$ and $x_1^2x_2$, which means that the number of coefficient we used is now grows exponentially to the number independent variable 

\paragraph{Padé approximant}
A Pade approximant is an approximation of a function using rational polynomials.
An $[N/M]$ Pade approximant is formed of a Nth degree polynomial on the numerator and an Mth degree polynomial on the denominator:
\begin{equation}
    P(x)=\frac{a_0+a_1 x+a_2x^2+... a_Nx^N}{b_0+b_1 x+b_2x^2+... b_Mx^M}
\end{equation}
% https://mathworld.wolfram.com/PadeApproximant.html
This technique is developed by Henri Padé around year 1890.
% https://en.wikipedia.org/wiki/Pad%C3%A9_approximant
Padé approximant $\frac{ax^2+bx^3+...}{c+dx+...+x^6}$ making sure that $f(x)$ does not tend to infinity at large x, in this case tend to 1/x\\
Padé approximant does not have the same problem of using 
Padé approximant is superior to the Taylor series when describing function that contains poles. 
Also by dividing a polynomial by another, the Pade approximant prevents the function from diverging by letting $N<=M$
\subsection{Neural network}
After 150 years or so \\
Neuro network, $\frac{x}{1+a\abs{x}}$ using less indicator\\
Use a sum of such $\frac{x}{1+a\abs{x}}$ element, which resembles a hyperbolic tangent.
after 30 years\\
Neural Network is used to solve classification problems, sometimes regression problem.
Example of Neural network code:
\subparagraph*{Back Propagation}
Backpropagation is just a way of propagating the total loss back into the neural network to know how much of the loss every node is responsible for, and subsequently updating the weights in a way that minimizes the loss by giving the nodes with higher error rates lower weights, and vice versa.
\subparagraph*{Gradient decent}
    Slowly updating parameters 
% https://towardsdatascience.com/deep-neural-networks-for-regression-problems-81321897ca3
\subparagraph*{Activation functioon}
\begin{enumerate}
    \item Activation function: takes in a real vale and spit out a value between (0 to 1) to add linearity to the network.
    %Paraphrase the below item%
    \item Activation functions introduce an additional step at each layer during the forward propagation, but its computation is worth it. Here is why—
    In that case, every neuron will only be performing a linear transformation on the inputs using the weights and biases. It’s because it doesn’t matter how many hidden layers we attach in the neural network; all layers will behave in the same way because the composition of two linear functions is a linear function itself.
    \item Different Types of activation function.

\end{enumerate}
\subparagraph*{Loss function}
% https://www.linkedin.com/posts/damienbenveniste_machinelearning-datascience-artificialintelligence-activity-7028762585705451520-iqtz/?utm_source=share&utm_medium=member_ios
\paragraph{Deep Neural network}
Deep neural network, layers of sum of indicator
the deepness refers to the layers\\
Layers can be seen as $f(f'(f''(x)))$\\

\section{Different methods in Machine learning}
The most natural and commonly used machine learning technique is classification and regression
\subsection{Gaussian Process}
COvariant 
\subsection{Neural network}
\subsection{Deep learning}
% This article provided a good insight of deep learning:
% https://www.sciencedirect.com/science/article/pii/S0370269317310390
It requires very large amount of data in order to perform better than other techniques. It is extremely expensive to train due to complex data models.
\subsection{Regression}
\subsection{Classification}
The output is discrete, can be more than 2.\\
Classification does give step function\\ not differentiable. 
\paragraph{Support Vector Regression}
% https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2
Instead of minim sing the error, SVR give us the flexibility to define how much error is tolerable and will find an appropriate line. The term we are minimising is the efficient vector while we keep the error as a constraint.(Hypersurface)
Non-linear Classification.
\paragraph{Decision Trees}
%https://link.springer.com/content/pdf/10.1007/BF00116251.pdf

Decision trees are used for two main types: classification tree and regression tree. We shall discuss the latter in this article as in physics, we usually expect a numerical outcome.

Random forest/Gaussian etc.
\paragraph{Random forest}
%https://web.archive.org/web/20160417030218/http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf
\paragraph*{KNN}
% https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761
K-nearest Neighbours
\paragraph*{SVN}
Non-linear problem
\section{Real life example}
\subsection{Gaussian Process}
% this is article that uses Gaussian process to We use simple activity models to investigate the relationship between the physical processes generating the signals and the covariances typically found in data, and to demonstrate the qualitative behaviour of this relationship.
% https://www.aanda.org/articles/aa/abs/2021/01/aa39594-20/aa39594-20.html

A rich article explaining a modified version of GP, which can mentioned previous researching in application of GP
https://www.sciencedirect.com/science/article/pii/S0191261521000369

\subsection{Gravitational Waves}
Problem 
metric that doesn't changed when shift or 
coefficient  shift-free and dimension free
$R^2$

$r^2$ different things.
\section{Choosing ML techniques}
Testing protocal 
Train validate test 5 fold. 
Tranlational text recogonisation. 

Take fouriour transform for symmetery
Translationall invarianct.

not invariant forest,
invariant covolution neural network.

Gaussian more perfect,

\section{Conclusion}
Put exam bullet point in conclusion.
Also, show future prospect, big advancement, is physics ready machine learning, what is the data collection like in physics

MCC for categorical data.
\begin{example}
    {Overall structure of this review.}{The over structure should be general concepts about machine learning followed by specific example.}
\end{example}
\begin{example}
    {How deep should I look into each case. For example, different activation function for neural network, different definition of distance in KNN problem, different definition of Loss function in liner regression?}{}
\end{example}
\begin{example}
    {Marking Scheme:}{communication/ Would they read my report, if yes, methods/how to next step}
\end{example}


Sum of F(ax+b)+F(ax+B) more superior to the Parday when no fit data with lesser parameters.




independently tested.
\end{document}