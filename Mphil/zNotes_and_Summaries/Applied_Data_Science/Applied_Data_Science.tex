\documentclass[12pt,a4paper]{article}

\usepackage{import}
\import{../Template/}{format.tex}

\newcommand{\topic}{Applied Data Science}

\begin{document}

\title{\topic}
\begin{titlepage}
    \maketitle
\end{titlepage}

\tableofcontents

\newpage
\begin{abstract}
\noindent
Abstract of this course
\end{abstract}
\section{Pre-processing Data}
\section{Understanding the structure of Data}
\begin{itemize}
    \item type of features: continuous, categorical
	\item ranges of features: [min, max], number of categories
	\item missing information [la/bels, features]
	\item discriminative power of features (redundancy)
\end{itemize}
\begin{theorem}
    {Simpson's paradox}
    {Simpson's paradox is observed in probability and statistics; a trend appears in several groups of data but disappears or reverses when the groups are combined.}
\end{theorem}

\subsection{Adjusting data without tampering with signal}
\subsubsection{Expression ranges and One-hot encoding}
For categorical data, we assign value by using one-hot encode which are the n unit vectors, which are equal distance from each other, from n-dimensional space to represent n categories
\subsubsection{Standardisation vs scaling}
There are different ways of scaling, the most straightforward one is linear scaling, others include log scaling.
Note that outliers will seriously affect scaling, it can be done by cap our value, however, it also means that we throw away data points
Note that we lose certain information when standardise, i.e. for Z 
$$
Z=\frac{x_i-\mu}{\sigma}
$$
you will lose the standard deviation.
Also, $\mu$ and $\sigma$ can be influenced by outliers. Median and MAD(median absolute deviation)are not affected by outliers
\begin{align}
    Z_{\text {med }} & =\frac{x_i-\text { median }}{M A D} \\
    M A D & =\text { median }\left(\mid x_i-\text { median } \mid\right)
\end{align} 
\subsubsection{Near zero variance}
Near zero variance means that the variable has little variance i.e. almost constant
\subsubsection{Multi-collinearity}
Multi-collinearity is a concept where independent variables are highly correlated. i.e. correlation coefficient = 1
We use PCA analysis to reduce the dimension of the highly correlated variable space 
\subsubsection{Dimensionality reduction}

\subsection{Engineering Model Robustness}
\begin{itemize}
    \item we draw the samples independently and identically (iid) at random from the distribution
    (there is no underlying structure that is present in the data)
    \item the sets are disjunct partitions of the original distribution
    (no intersection between training set and test set)
    the size of the validation and test sets should be comparable (if not identical)
    \item The validation set should be large enough to detect differences between models
    \item accuracy is not the only metric to measure the performance of the model
    \item  on the test set, the error between the prediction and the actual label is the test error
    \item the objective function of the algorithm minimizes the test errors by parameter tuning
    \item Models are further evaluated for Bias and Variance (assessment of overfitting/ underfitting)
\end{itemize}
\subsubsection{Validation set}
k-fold validation is essentially k models
ALSO use multiple k, iterations
Usually 
\subsubsection{Confusion matrix}
Definition of confusion matrix
A stacked approach to deal with data with underlying substructure.
choose a representation? PCA
o weighted summary
\subsubsection{Unbalanced data}
fixed by upweighting or down sampling
\begin{theorem}
    {Nyquistâ€“Shannon sampling theorem}{}
\end{theorem}
\begin{theorem}
    {Kullback-Leiber divergence (per classes or using a binning approach for continuous data)
    }{}
\end{theorem}
\section{Supervised Learning: Regression}
We assume the model
$$
Y=\beta_0+\beta_1 X+\epsilon
$$
where $\beta_0$ and $\beta_1$ are two unknown constants that represent the intercept and the slope; $\beta_0$ and $\beta_1$ are also known as coefficients or parameters and $\epsilon$ is the error term.

Given the estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ for the model coefficients, we predict the output, $\hat{y}$ using
$$
\hat{y}=\hat{\beta}_0+\hat{\beta}_1 x
$$
where $\hat{y}$ indicates the prediction of $Y$ on the basis of $X=x$. The hat symbol denotes an estimated value.
\subsection{Standard errors}
\subsection{Hypothesis Testing}
Standard errors can be used to perform hypothesis testing on the coefficients. The most common hypothesis test involves testing the null hypothesis of:
$H_0:$ There is no relationship between $X$ and $Y$
$H_1:$ There is some relationship between $X$ and $Y$
Or, more formally:
$$
\begin{aligned}
& H_0: \quad \beta_1=0 \\
& H_1: \quad \beta_1 \neq 0
\end{aligned}
$$
To test the $H_0$ we compute a t-statistic:
$$
t=\frac{\hat{\beta_1}-0}{S E\left(\hat{\beta}_1\right)}
$$

This will be a t-distribution with $n-2$ degrees of freedom.
Using $R$, we can compute the probability of observing any value $\geq|t|$. We call this probability $\mathbf{p}$-value.
\subsection{Type of Loss Functions}
\begin{itemize}
    \item Mean Absolute Error (MAE) (L1 Loss)
    \item Mean Squared Error (MSE) (L2 Loss)
    \item Mean Biased Error (MBE)
    \item Hubber Loss (L1-L2 Loss)
\end{itemize}
\subsection{Bias and Variance Trade-off}
\textbf{Bias:} the model's error rate on the training set (rephrased, the difference between the average prediction and the correct value we are predicting).

A model with high bias is oversimplified (insufficient information acquired from the training data).

\textbf{Variance: }the model's error rate on the validation (or test) set, in addition to the bias

A model with high variance captures the signal and the noise in the training data and fails to generalise well on (unseen) test data.
\subsection{Multiple (linear) regression. Model selection}
How to choose which subsets to use, for $n$ features, there are $2^n$ subsets, which is not feasible to try all of them.
\subsection{Model selection.}
\paragraph{Forward Selection}
Begin with the null model, a model that contains an intercept but no predictors\\
-Fit p simple linear regressions, each with only one feature and add to the null model the variable that results in the lowest RSS.\\
-Add to that model the variable that results in the lowest RSS amongst all two-variable models.\\
- Continue until some stopping rule is satisfied, for example when all remaining variables have a $p$-value above some threshold
\paragraph{backward selection}

Start with all variables in the model; fit the model

Remove the variable with the largest p-value i.e. the variable that is the least statistically significant

The new $p-1$-variable model is fit, and the variable with the largest p-value is removed.

Continue until a stopping rule is reached e.g. when all remaining variables have a significant $p$-value above some threshold
\subsection{Parametric logistic regression}
\subsection{Non-parametric regression}
Decision tree, where the data space is partitioned into regions

The complexity parameter prevents the tree from having too many branches, which is prone to overfitting
\subsection{Regularisation}
Regularisation is the process of adjusting an algorithm to prefer a smaller model, to avoid overfitting. This is done by modifiying the loss function to include a penalty for large weights.


\section{Machine Learning Model - KNN}
K nearest neighbour
\begin{itemize}
    \item KNN is a non-parametric method
    \item KNN distance can be defined with Eucleadian distance, Manhattan distance, etc.s
    \item Lower K means a more complex model (Prone to overfitting)
    \item Higher K means a less complex model (Prone to underfitting). 
    In the most extreme case, K = N, then your model would predict the same outcome for all data points.
\end{itemize}
\subsection{Standardisation}
    Standardisation is necessary as we need to make sure distances are equally weighted. Another method is to use correlation-based distance.
    Check the skewness before Standardisation

\section{Machine Learning Model Support Vector Machine}
    There is always a misclassification
    Transform points to higher dimensions to allow linear separation

\section{Machine Learning Model - Decision Tree}
    A decision tree is a non-parametric method, therefore it does not require data pre-processing.
    \subsection{Recursive binary partitions}
        Resoureces: An Introduction to Statistical learning with Applications in R/Python
        Segment feature space into simple regions (high demension rectangles)
        Predict usinging the average, mode, classify using the most common class
    \subsection{Characteristic of a decision tree}
        \begin{itemize}
            \item branches: represent the decision rules
            \item interial nodes:
            \item leaf nodes (terminal nodes): represent the outcome, 
        \end{itemize}

    \subsection{How to grow a tree}
    \paragraph{}
    How to grow the tree?
    - General process:
    1. Partition the predictor space (i.e. all possible values of $X_1, X_2, \ldots, X_p$ ) into $J$ distinct, non-overlapping regions, labelled $R_1, \ldots, R_J$
    2. Each observation in a given region $R_f$ is given the same predicted value (or class), which is the mean (or mode) of all response variables in that region
    - How do we choose the partitions? e.g. high-dimensional rectangles
    \paragraph*{Greedy approach - regression}
        regression trees
        - For one of the predictors $X_j$ with a cutoff threshold $c$, we define two regions:
    $$
    R_1(j, c)=\left\{X \mid X_j<c\right\} \text { and } R_2(j, c)=\left\{X \mid X_j \geq c\right\}
    $$
    - An observation $x_i$ is in region $R_1(j, c)$ if $x_i<c$ and in region $R_2(j, c)$ if $x_{i j} \geq c$
    - For the first iteration, $R_1(j, c)$ and $R_2(j, c)$ partitions the entire predictor space
    - For tater tterations, $R_1(j, c)$ and $R_2(j, c)$ partitions a previous parent' region (so inherits any previous conditions of that 'parent' region)
    \paragraph{Classification}
    The function to minimise is to reduce the Gini-index,
\subsection{Bagging}
Bagging is the process of creating multiple models from different subsets of the training data, of the same size, by bootstraping. The final prediction is averaged across (majority vote or average across) the predictions of all of the sub-models.
\subsection{Random Forest}
Random forest is an ensemble method that combines multiple decision trees. It is a bagging method with a random selection of features. The final prediction is averaged across the predictions of all of the sub-models.
\subsection{Boosting}
Boosting sequentially builds a model by training a decision tree on the residuals of the previous model. The final prediction is the sum of the predictions of all of the sub-models. The parameters include the learning rate, which is the shrinkage factor of the model for the residuals.

\section{Machine Learning Model - K-MEAN}
K-mean is a clustering method, which is an unsupervised learning method. It is used to group data points into clusters, where the data points in the same cluster are similar to each other, and the data points in different clusters are dissimilar to each other.

K-mean clustering are widely used in pattern recognition, computer vision, etc.

K-mean is similar to k-nearest neighbour.

K-mean is a NP-hard problem, which means that it is computationally expensive to find the optimal solution. Therefore, we use a heuristic approach to find a sub-optimal solution.
\subsection{Algorithm}
\subsubsection{Lloydâ€™s algorithm (naive k-means)}
Steps of Lloydâ€™s algorithm:
\begin{enumerate}
    \item Initialisation: Randomly assign each data point to one of the $k$ clusters
    \item Assign each data point to the closest centroid
    \item Compute the centroid of each cluster
    \item Repeat steps 2 and 3 until the centroids do not change
\end{enumerate}
This algorithm should reduce the within-cluster sum of squares (WCSS) at each iteration.
\subsubsection{K-means alternatives}
\begin{itemize}
    \item K-median: use the median instead of the mean to compute the centroid
    \item K-medoids: use the most central point in the cluster as the centroid
    \item Manhattan distance: use Manhattan distance instead of Euclidean distance to compute the distance between a data point and a centroid (i.e. $d(x, y)=\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|$)
\end{itemize}
\subsubsection{Initialisation}
There are serveral ways to initialise the centroids:
\begin{itemize}
    \item Random partition: Randomly assign each data point to one of the $k$ clusters
    \item Forgy: Randomly select $k$ data points as the initial centroids
    \item K-means++: select the first centroid randomly, then select the next centroid from the remaining data points with a probability proportional to the distance from the previous centroid
\end{itemize}

\subsection{Fuzzy k-means}
\begin{enumerate}
    \item Multiple cluster assignment: $x_i$ has cluster assignment $w_{i k}$, with $w_{i k}^{-1}=\sum_{j=1}^K\left(\frac{\left\|x_i-c_k\right\|}{\left\|x_i-c_j\right\|}\right)^{\frac{2}{m-1}}$
    \item Centroid update
- New cluster centroids are $c_k=\frac{\sum_{i=1}^n w_{i k}^m x_i}{\sum_{i=1}^n w_{i k}^m}$
- This minimises the weighted mean squared error $E=\sum_{i=1}^n \sum_{k=1}^K w_{i k}^m\left\|x_i-c_k\right\|^2$
- $m$ is a fuzziness hyper parameter
\end{enumerate}

\subsection{Types of clustering}
\begin{itemize}
    \item Hard vs soft clustering
    \item Hierarchical vs non-Hierarchical clustering
    \item AgglomeraCve vs partitioning/divisive
    \item Centroid vs distribuCon-based vs density vs graph-based vs spectral
\end{itemize}

\section{Clustering}
\subsection{GMM}
\subsection{Density}
\subsection{Evaluation}
The challenge in evaluating clustering is that quantitative measures are not available.
There is internal evaluation and external evaluation.

\section{Python Library}
\subsection{statsmodels}
    \paragraph{statsmodels.api}
    https://www.statsmodels.org/stable/examples/index.html
\subsection{sklearn}
    \subparagraph{neighbours.KNeighborsClassifier}
    Parameters:
        \begin{itemize}
            \item n\_neighbors: number of neighbours
            \item weights: uniform, distance
            \item algorithm: auto, ball\_tree, kd\_tree, brute. Trees reduces the complexity from $O(N)$ to $O(logN)$
            \item leaf\_size: leaf size for ball\_tree or kd\_tree
            \item p: power in minkowski distance
        \end{itemize}
    \subparagraph{output}
        conf\_matrix = confusion\_matrix(y\_test, y\_pred)
        Cohen's Kappa: $\kappa=\frac{p_o-p_e}{1-p_e}$, where $p_o$ is the accuracy and $p_e$ is the expected accuracy, we want to maximum Kappa.
        The definition of $p_e$ is:
        $$
        P_e=\frac{\sum\left(\frac{\text { Row sum for category } i \times \text { Column sum for category } i}{\text { Total number of observations }}\right)}{\text { Total number of observations }}
        $$
        
        To simplify $P_e$, we can rearrange the terms:
        $$
        P_e=\frac{\sum(\text { Row sum for category } i \times \text { Column sum for category } i)}{(\text { Total number of observations })^2}
        $$
    \subparagraph{neighbours.KNeighborsRegressor}
    
\end{document}
