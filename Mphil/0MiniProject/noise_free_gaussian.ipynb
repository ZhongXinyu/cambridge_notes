{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gpjax.mean_functions.Zero'>\n",
      "<class 'gpjax.kernels.stationary.rbf.RBF'>\n",
      "<class 'gpjax.gps.Prior'>\n",
      "<class 'gpjax.gps.ConjugatePosterior'>\n"
     ]
    }
   ],
   "source": [
    "import gpjax as gpx\n",
    "\n",
    "mean = gpx.mean_functions.Zero()\n",
    "kernel = gpx.kernels.RBF()\n",
    "prior = gpx.gps.Prior(mean_function = mean, kernel = kernel)\n",
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints = 123)\n",
    "\n",
    "posterior = prior * likelihood\n",
    "\n",
    "\n",
    "print (type(mean))\n",
    "print (type(kernel))\n",
    "print (type(prior))\n",
    "print (type(posterior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Kernel function\n",
    "\n",
    "def Kernel(x1, x2):\n",
    "    \"\"\"\n",
    "    Gaussian Kernel function\n",
    "    \"\"\"\n",
    "    return np.exp(-0.5 * np.sum((x1 - x2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a GP $f(\\cdot) \\sim \\mathcal{GP}(\\mu(\\cdot), k(\\cdot, \\cdot))$ with mean function $\\mu: \\mathcal{X} \\rightarrow \\mathbb{R}$ and $\\boldsymbol{\\theta}$-parameterised kernel $k: \\mathcal{X} \\times \\mathcal{X}\\rightarrow \\mathbb{R}$. When evaluating the GP on a finite set of points $\\mathbf{X}\\subset\\mathcal{X}$, $k$ gives rise to the Gram matrix $\\mathbf{K}_{ff}$ such that the $(i, j)^{\\text{th}}$ entry of the matrix is given by $[\\mathbf{K}_{ff}]_{i, j} = k(\\mathbf{x}_i, \\mathbf{x}_j)$. As is conventional within the literature, we centre our training data and assume $\\mu(\\mathbf{X}):= 0$ for all $\\mathbf{X}\\in\\mathbf{X}$. We further drop dependency on $\\boldsymbol{\\theta}$ and $\\mathbf{X}$ for notational convenience in the remainder of this article.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a joint GP prior over the latent function\n",
    "\n",
    "\\begin{align}\n",
    "p\\left(\\mathbf{f}, \\mathbf{f}^{\\star}\\right)=\\mathcal{N}\\left(\\mathbf{0},\\left[\\begin{array}{ll}\n",
    "\\mathbf{K}_{x f} & \\mathbf{K}_{x x}\n",
    "\\end{array}\\right]\\right)\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{f}^{\\star} = f(\\mathbf{X}^{\\star})$. Conditional on the GP's latent function $f$, we assume a factorising likelihood generates our observations\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{y} \\mid \\mathbf{f})=\\prod_{i=1}^n p\\left(y_i \\mid f_i\\right)\n",
    "\\end{align}\n",
    "\n",
    "Strictly speaking, the likelihood function is $p(\\mathbf{y}\\,|\\,\\phi(\\mathbf{f}))$ where $\\phi$ is the likelihood function's associated link function. Example link functions include the probit or logistic functions for a Bernoulli likelihood and the identity function for a Gaussian likelihood. We eschew this notation for now as this section primarily considers Gaussian likelihood functions where the role of $\\phi$ is superfluous. However, this intuition will be helpful for models with a non-Gaussian likelihood, such as those encountered in classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Bayes' theorem (???) yields the joint posterior distribution over the latent function\n",
    "\\begin{equation*}\n",
    "p\\left(\\mathbf{f}, \\mathbf{f}^{\\star} \\mid \\mathbf{y}\\right)=\\frac{p(\\mathbf{y} \\mid \\mathbf{f}) p\\left(\\mathbf{f}, \\mathbf{f}^{\\star}\\right)}{p(\\mathbf{y})} .\n",
    "\\end{equation*}\n",
    "\n",
    "The choice of kernel function that we use to parameterise our GP is an important modelling decision as the choice of kernel dictates properties such as differentiability, variance and characteristic lengthscale of the functions that are admissible under the GP prior. A kernel is a positive-definite function with parameters $\\boldsymbol{\\theta}$ that maps pairs of inputs $\\mathbf{X}, \\mathbf{X}' \\in \\mathcal{X}$ onto the real line. We dedicate the entirety of the Introduction to Kernels notebook to exploring the different GPs each kernel can yield."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
